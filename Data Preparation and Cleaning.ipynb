{"cells":[{"cell_type":"markdown","source":["### **AdventureWorks2022 Project**\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1a6de58e-1b42-4a82-9ca5-17e75be1557d"},{"cell_type":"markdown","source":["### Section 1: Preview and Schema Inspection"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2e1f5c91-2de5-4174-9853-fb2e0f763335"},{"cell_type":"code","source":["# Path to folder containing raw files in Lakehouse\n","folder_path = \"Files/raw/\"\n","\n","# List of files to inspect\n","files = [\n","    \"AdventureWorks Customer Lookup.csv\",\n","    \"AdventureWorks Product Lookup.csv\",\n","    \"AdventureWorks Calendar Lookup.csv\",\n","    \"AdventureWorks Product Categories Lookup.csv\",\n","    \"AdventureWorks Product Subcategories Lookup.csv\",\n","    \"AdventureWorks Territory Lookup.csv\",\n","    \"AdventureWorks Returns Lookup.csv\",\n","    \"AdventureWorks Sales Data 2020.csv\",\n","    \"AdventureWorks Sales Data 2021.csv\",\n","    \"AdventureWorks Sales Data 2022.csv\"\n","]\n","\n","# Display schema and sample rows for each file\n","for file in files:\n","    print(f\"\\nüìÇ Inspecting file: {file}\")\n","    df = (\n","        spark.read\n","        .option(\"header\", \"true\")\n","        .option(\"inferSchema\", \"true\")\n","        .csv(folder_path + file)\n","    )\n","    df.printSchema()\n","    df.show(5, truncate=False)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c4bc37c9-f3fc-4f6c-ba29-f11f7b62d1e2"},{"cell_type":"markdown","source":[" ### Section 2: ETL ‚Äì Load and Transform to Tables"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"284415e1-db4d-438c-9d8e-9c00cf8fb3d8"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Path to folder containing raw files\n","folder_path = \"Files/raw/\"\n","\n","# Mapping of file names to destination Lakehouse tables\n","files_and_tables = {\n","    \"AdventureWorks Customer Lookup.csv\": \"Dim_Customer\",\n","    \"AdventureWorks Product Lookup.csv\": \"Dim_Product\",\n","    \"AdventureWorks Calendar Lookup.csv\": \"Dim_Calendar\",\n","    \"AdventureWorks Product Categories Lookup.csv\": \"Dim_ProductCategory\",\n","    \"AdventureWorks Product Subcategories Lookup.csv\": \"Dim_ProductSubcategory\",\n","    \"AdventureWorks Territory Lookup.csv\": \"Dim_Territory\",\n","    \"AdventureWorks Returns Lookup.csv\": \"Fact_Returns\",\n","    \"AdventureWorks Sales Data 2020.csv\": \"Stg_Sales_2020\",\n","    \"AdventureWorks Sales Data 2021.csv\": \"Stg_Sales_2021\",\n","    \"AdventureWorks Sales Data 2022.csv\": \"Stg_Sales_2022\"\n","}\n","\n","# Columns to cast to string (by file)\n","columns_to_string = {\n","    \"AdventureWorks Customer Lookup.csv\": [\"CustomerKey\"],\n","    \"AdventureWorks Product Lookup.csv\": [\"ProductKey\", \"ProductSubcategoryKey\"],\n","    \"AdventureWorks Returns Lookup.csv\": [\"TerritoryKey\", \"ProductKey\"],\n","    \"AdventureWorks Product Categories Lookup.csv\": [\"ProductCategoryKey\"],\n","    \"AdventureWorks Product Subcategories Lookup.csv\": [\"ProductSubcategoryKey\", \"ProductCategoryKey\"],\n","    \"AdventureWorks Territory Lookup.csv\": [\"SalesTerritoryKey\"]\n","}\n","\n","# Process each file\n","for file_name, table_name in files_and_tables.items():\n","    print(f\"\\nüì• Processing file: {file_name} ‚Üí saving as table: {table_name}\")\n","\n","    df = (\n","        spark.read\n","        .option(\"header\", \"true\")\n","        .option(\"inferSchema\", \"true\")\n","        .csv(folder_path + file_name)\n","    )\n","\n","    # Cast specified columns to string\n","    if file_name in columns_to_string:\n","        for col_name in columns_to_string[file_name]:\n","            df = df.withColumn(col_name, col(col_name).cast(\"string\"))\n","        print(f\"üî§ Converted keys to string: {columns_to_string[file_name]}\")\n","\n","    # Use overwrite for sales data, append for others\n","    mode = \"overwrite\" if \"Sales\" in file_name else \"append\"\n","\n","    df.write.mode(mode).saveAsTable(table_name)\n","    print(f\"‚úÖ Table saved: {table_name} (mode = {mode})\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"614c7aec-065e-4e65-9667-9353721ae11e"},{"cell_type":"markdown","source":[" ### Section 3: Add FullName to Dim_Customer"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"96a2b3a7-f112-4351-bf84-f518591f3aa9"},{"cell_type":"code","source":["from pyspark.sql.functions import concat_ws, col\n","\n","# Load customer file\n","file_path = \"Files/raw/AdventureWorks Customer Lookup.csv\"\n","df = spark.read.option(\"header\", True).csv(file_path)\n","\n","# Filter out rows with null CustomerKey\n","df_cleaned = df.filter(col(\"CustomerKey\").isNotNull())\n","\n","# Add FullName column\n","df_with_fullname = df_cleaned.withColumn(\"FullName\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\")))\n","\n","# Overwrite existing table\n","df_with_fullname.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"Dim_Customer\")\n","print(\"‚úÖ Dim_Customer updated with FullName column.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be8addd1-0e6b-4ffc-b9b4-839c50e0130c"},{"cell_type":"markdown","source":["‚úÖ Summary of Actions Performed in the Notebook"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe84582c-a99c-4ff6-b87e-d671d95bdf9e"},{"cell_type":"markdown","source":["| Step | Description                                                              | Tool/Technique Used                            |\n","| ---- | ------------------------------------------------------------------------ | ---------------------------------------------- |\n","| 1Ô∏è‚É£  | Inspected the structure and schema of raw CSV files                      | PySpark `DataFrame.printSchema()` and `show()` |\n","| 2Ô∏è‚É£  | Loaded raw CSV files into Lakehouse                                      | PySpark `read.csv()`                           |\n","| 3Ô∏è‚É£  | Casted key columns to string data type where needed                      | `withColumn(...cast(\"string\"))`                |\n","| 4Ô∏è‚É£  | Saved transformed data to Lakehouse tables using appropriate write modes | `write.mode(...).saveAsTable()`                |\n","| 5Ô∏è‚É£  | Cleaned customer data and added `FullName` column                        | `filter()`, `concat_ws()`                      |\n","| 6Ô∏è‚É£  | Overwrote the existing `Dim_Customer` table with enriched data           | `write.mode(\"overwrite\").saveAsTable()`        |\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"029298a0-b890-4934-91bf-5232b832aa19"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"8350f44d-fbe4-4d64-afbd-57a049fe4a57","known_lakehouses":[{"id":"8350f44d-fbe4-4d64-afbd-57a049fe4a57"}],"default_lakehouse_name":"AdventureWorks2022Lakehouse","default_lakehouse_workspace_id":"7503d12e-9687-4aef-903b-52c0c140da85"}}},"nbformat":4,"nbformat_minor":5}